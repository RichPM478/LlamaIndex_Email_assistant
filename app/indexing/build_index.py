# app/indexing/build_index.py
import os, glob, json
from typing import List, Dict, Any, Optional
from llama_index.core import VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import TextNode
from datetime import datetime

from app.config.settings import get_settings
from app.llm.provider import configure_llm
from app.embeddings.provider import configure_embeddings


def _load_raw_emails(path: str) -> List[Dict[str, Any]]:
    """Load emails from .json, .jsonl, or .json.enc (encrypted) files."""
    from app.security.encryption import credential_manager
    
    emails: List[Dict[str, Any]] = []
    
    try:
        # Handle encrypted files
        if path.endswith(".json.enc"):
            print(f"ðŸ”“ Decrypting email file: {path}")
            with open(path, "r", encoding="utf-8") as f:
                encrypted_content = f.read().strip()
            
            # Decrypt the content
            decrypted_content = credential_manager.decrypt_credential(encrypted_content)
            data = json.loads(decrypted_content)
            
            if isinstance(data, list):
                emails = data
            else:
                raise ValueError(f"{path} must contain a JSON list")
                
        elif path.endswith(".jsonl"):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        emails.append(json.loads(line))
                        
        elif path.endswith(".json"):
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, list):
                    emails = data
                else:
                    raise ValueError(f"{path} must contain a JSON list")
        else:
            raise ValueError(f"Unsupported raw format: {path}")
            
        print(f"ðŸ“§ Successfully loaded {len(emails)} emails from {path}")
        return emails
        
    except Exception as e:
        print(f"âŒ Failed to load emails from {path}: {e}")
        raise


def _resolve_latest_raw(explicit_path: Optional[str]) -> str:
    """Pick the explicit file, or newest file in data/raw/, by modified time (supports encrypted files)."""
    if explicit_path and os.path.exists(explicit_path):
        return explicit_path

    # Look for both encrypted and unencrypted files
    candidates = (glob.glob("data/raw/*.json") + 
                  glob.glob("data/raw/*.jsonl") + 
                  glob.glob("data/raw/*.json.enc"))
    
    if not candidates:
        raise FileNotFoundError("No raw files found in data/raw/. Run `python main.py ingest` first.")

    latest = max(candidates, key=os.path.getmtime)
    return latest


def _normalize_sender(sender: str) -> str:
    """Normalize sender field for better matching"""
    if not sender:
        return "unknown"
    
    # Extract just the name/org part, remove email addresses
    sender = sender.lower()
    
    # Remove email parts like <email@domain.com>
    import re
    sender = re.sub(r'<[^>]+>', '', sender).strip()
    
    # Remove quotes
    sender = sender.replace('"', '').replace("'", '')
    
    return sender.strip()


def build_index(raw_path: Optional[str] = None, persist_dir: str = "data/index") -> VectorStoreIndex:
    settings = get_settings()
    configure_llm(settings)
    configure_embeddings(settings)

    raw_file = _resolve_latest_raw(raw_path)
    print(f"[index] Using raw file: {raw_file}")

    emails = _load_raw_emails(raw_file)
    print(f"[index] Processing {len(emails)} emails")

    # Use smaller chunks for better precision
    splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    nodes: List[TextNode] = []
    
    # Track unique senders for debugging
    unique_senders = set()
    
    for i, e in enumerate(emails):
        text = (e.get("body_text") or e.get("body") or "").strip()
        if not text:
            continue
        
        # Use clean sender field if available, fallback to original
        sender = e.get("sender") or e.get("from_") or e.get("from") or "unknown"
        sender_normalized = _normalize_sender(sender)
        unique_senders.add(sender_normalized)
        
        # Enhanced metadata with normalized fields
        meta = {
            "message_id": e.get("message_id"),
            "uid": e.get("uid"),
            "subject": e.get("subject") or "No subject",
            "from": sender,  # Keep original
            "from_normalized": sender_normalized,  # Add normalized version
            "date": e.get("date"),
            "sent_at": e.get("sent_at"),
            "folder": e.get("folder"),
            "email_index": i  # Track which email this came from
        }
        
        # Create a more informative text representation
        # Include metadata in the text for better semantic matching
        enhanced_text = f"From: {sender}\nSubject: {e.get('subject', 'No subject')}\n\n{text}"
        
        # Split into chunks
        chunks = splitter.split_text(enhanced_text)
        
        # If no chunks (very short email), create at least one node
        if not chunks and text:
            chunks = [enhanced_text]
        
        for chunk_idx, ch in enumerate(chunks):
            node_meta = meta.copy()
            node_meta["chunk_index"] = chunk_idx
            node_meta["total_chunks"] = len(chunks)
            nodes.append(TextNode(text=ch, metadata=node_meta))
    
    print(f"[index] Created {len(nodes)} nodes from {len(emails)} emails")
    print(f"[index] Unique senders found: {len(unique_senders)}")
    print(f"[index] Sample senders: {list(unique_senders)[:10]}")

    os.makedirs(persist_dir, exist_ok=True)
    
    # Create index with explicit settings
    index = VectorStoreIndex(
        nodes,
        show_progress=True,
    )
    
    index.storage_context.persist(persist_dir=persist_dir)
    print(f"[index] Index saved to {persist_dir}")
    
    return index
